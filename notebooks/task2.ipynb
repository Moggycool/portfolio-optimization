{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "186a3ca3",
            "metadata": {},
            "source": [
                "# Task 2 — TSLA Time Series Forecasting (ARIMA/SARIMA vs Multivariate LSTM)\n",
                "\n",
                "This notebook is the **only** notebook for Task 2. **All modeling and artifact generation is done via scripts** (see `scripts/04_task2_train_arima.py`, `scripts/05_task2_train_lstm.py`, `scripts/06_task2_compare_models.py`).\n",
                "\n",
                "## Required deliverables shown here\n",
                "1. **Chronological split** with cutoff on the **last trading day of 2024** (loaded from `split_info.json`).\n",
                "2. **ARIMA/SARIMA model specification** (loaded from `arima_params.json`).\n",
                "3. **LSTM architecture + training configuration** (loaded from `lstm_architecture.json`).\n",
                "4. **Forecast CSVs aligned to test dates** (`tsla_arima_forecast.csv`, `tsla_lstm_forecast.csv`, `tsla_forecasts_merged.csv`).\n",
                "5. **Performance metrics table** with MAE/RMSE/MAPE (`model_comparison.csv`).\n",
                "6. A brief **discussion**: which model performed better and why, grounded in the metrics and behavior.\n",
                "\n",
                "---\n",
                "\n",
                "## Additional (demonstration) workflow sections\n",
                "To make the end-to-end methodology transparent, this notebook also **demonstrates** the standard time series workflow steps using the saved split/feature datasets (created by scripts):\n",
                "\n",
                "### A) Classical time series forecasting workflow (ARIMA/SARIMA)\n",
                "1. Load TSLA time series\n",
                "2. Explore and understand the data\n",
                "3. Test stationarity (ADF)\n",
                "4. **Demo:** build an ARIMA model on *returns* (stationary)\n",
                "5. Forecast prices (via returns → price path)\n",
                "6. Visualize and interpret results\n",
                "\n",
                "### B) Deep learning workflow (LSTM)\n",
                "1. Load TSLA time series\n",
                "2. Explore and understand the data\n",
                "3. Test stationarity (ADF)\n",
                "4. **Demo:** prepare data for LSTM (scaling + sequence creation)\n",
                "5. (Training is done in scripts)\n",
                "6. Forecast prices (loaded from artifacts)\n",
                "7. Visualize and interpret results\n",
                "8. Evaluate forecast accuracy\n",
                "\n",
                "> Important: the **official submission outputs** remain the script-generated artifacts in `outputs/task2/*`.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "7528c10e",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Standard imports\n",
                "import os\n",
                "import json\n",
                "import warnings\n",
                "from pathlib import Path\n",
                "\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "# Display settings\n",
                "pd.set_option('display.max_columns', 200)\n",
                "pd.set_option('display.width', 120)\n",
                "\n",
                "def _find_repo_root(start: Path) -> Path:\n",
                "    \"\"\"Walk upward until we find a folder containing both `src/` and `outputs/`.\"\"\"\n",
                "    start = start.resolve()\n",
                "    for candidate in [start, *start.parents]:\n",
                "        if (candidate / 'src').is_dir() and (candidate / 'outputs').exists():\n",
                "            return candidate\n",
                "    return start\n",
                "\n",
                "# Ensure REPO_ROOT is always a concrete Path\n",
                "_existing_repo_root = globals().get('REPO_ROOT', None)\n",
                "if isinstance(_existing_repo_root, Path):\n",
                "    REPO_ROOT = _existing_repo_root\n",
                "else:\n",
                "    REPO_ROOT = _find_repo_root(Path.cwd())\n",
                "    globals()['REPO_ROOT'] = REPO_ROOT\n",
                "    print('Repo root (auto-detected):', REPO_ROOT)\n",
                "\n",
                "print('Notebook working directory:', os.getcwd())\n",
                "print('Repo root:', REPO_ROOT)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "eedcf041",
            "metadata": {},
            "source": [
                "## 0) Define paths (repo-relative)\n",
                "\n",
                "These paths match the script outputs defined in `src/config.py`.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "fd92d43e",
            "metadata": {},
            "outputs": [],
            "source": [
                "# --- Script-generated artifacts (required deliverables) ---\n",
                "SPLIT_INFO_PATH = REPO_ROOT / 'outputs' / 'task2' / 'metrics' / 'split_info.json'\n",
                "ARIMA_PARAMS_PATH = REPO_ROOT / 'outputs' / 'task2' / 'metrics' / 'arima_params.json'\n",
                "LSTM_ARCH_PATH = REPO_ROOT / 'outputs' / 'task2' / 'metrics' / 'lstm_architecture.json'\n",
                "MODEL_COMPARISON_PATH = REPO_ROOT / 'outputs' / 'task2' / 'metrics' / 'model_comparison.csv'\n",
                "ERROR_DIAGNOSTICS_PATH = REPO_ROOT / 'outputs' / 'task2' / 'metrics' / 'error_diagnostics.csv'\n",
                "\n",
                "ARIMA_FORECAST_PATH = REPO_ROOT / 'outputs' / 'task2' / 'forecasts' / 'tsla_arima_forecast.csv'\n",
                "LSTM_FORECAST_PATH = REPO_ROOT / 'outputs' / 'task2' / 'forecasts' / 'tsla_lstm_forecast.csv'\n",
                "MERGED_FORECASTS_PATH = REPO_ROOT / 'outputs' / 'task2' / 'forecasts' / 'tsla_forecasts_merged.csv'\n",
                "\n",
                "FORECAST_PLOT_PATH = REPO_ROOT / 'outputs' / 'task2' / 'figures' / 'forecast_test_period.png'\n",
                "\n",
                "required_files = [\n",
                "    SPLIT_INFO_PATH,\n",
                "    ARIMA_PARAMS_PATH,\n",
                "    LSTM_ARCH_PATH,\n",
                "    MODEL_COMPARISON_PATH,\n",
                "    ERROR_DIAGNOSTICS_PATH,\n",
                "    ARIMA_FORECAST_PATH,\n",
                "    LSTM_FORECAST_PATH,\n",
                "    MERGED_FORECASTS_PATH,\n",
                "]\n",
                "\n",
                "missing = [p for p in required_files if not p.exists()]\n",
                "if missing:\n",
                "    print('Missing required artifacts. Run scripts first:')\n",
                "    for m in missing:\n",
                "        print(' -', m)\n",
                "else:\n",
                "    print('All required artifacts exist.')\n",
                "\n",
                "print('Optional plot exists:', FORECAST_PLOT_PATH.exists())\n",
                "\n",
                "# --- (NEW) Underlying split/feature datasets used for workflow demos ---\n",
                "TRAIN_SPLIT_PATH = REPO_ROOT / 'data' / 'task2' / 'splits' / 'tsla_train.parquet'\n",
                "TEST_SPLIT_PATH  = REPO_ROOT / 'data' / 'task2' / 'splits' / 'tsla_test.parquet'\n",
                "\n",
                "FEATURES_TRAIN_PATH = REPO_ROOT / 'data' / 'task2' / 'features' / 'tsla_features_train.parquet'\n",
                "FEATURES_TEST_PATH  = REPO_ROOT / 'data' / 'task2' / 'features' / 'tsla_features_test.parquet'\n",
                "\n",
                "optional_inputs = [TRAIN_SPLIT_PATH, TEST_SPLIT_PATH, FEATURES_TRAIN_PATH, FEATURES_TEST_PATH]\n",
                "missing_inputs = [p for p in optional_inputs if not p.exists()]\n",
                "print('Underlying split/feature files present:', len(missing_inputs) == 0)\n",
                "if missing_inputs:\n",
                "    print('Some optional inputs are missing (demo sections may be skipped):')\n",
                "    for p in missing_inputs:\n",
                "        print(' -', p)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "d387eef5",
            "metadata": {},
            "source": [
                "## Time Series Forecasting: ARIMA vs SARIMA vs LSTM (context)\n",
                "\n",
                "### Goal\n",
                "Forecast future values using patterns in past observations.\n",
                "\n",
                "### Models Compared\n",
                "- **ARIMA/SARIMA (Classical):** linear, interpretable, strong baselines\n",
                "- **LSTM (Modern):** non-linear, can learn complex temporal patterns and interactions among multiple features\n",
                "\n",
                "### Key Decision Factors\n",
                "- Seasonality present? (→ consider SARIMA)\n",
                "- Data size (small → classical often strong; larger + richer features → deep learning may help)\n",
                "- Interpretability needs\n",
                "\n",
                "### Workflow\n",
                "`Clean → Check stationarity/seasonality → Model → Evaluate`\n",
                "\n",
                "> In this project, the **official trained models and forecasts** are generated by scripts and loaded below as artifacts.\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "7a6cb981",
            "metadata": {},
            "source": [
                "## 1) Load split information (proof of correct chronological cutoff)\n",
                "\n",
                "Task requirement: chronological split where training ends on the **last trading day of 2024**.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "16650c71",
            "metadata": {},
            "outputs": [],
            "source": [
                "with SPLIT_INFO_PATH.open('r', encoding='utf-8') as f:\n",
                "    split_info = json.load(f)\n",
                "\n",
                "display(split_info)\n",
                "\n",
                "split_summary = pd.DataFrame([split_info])\n",
                "display(split_summary)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "69ab8773",
            "metadata": {},
            "source": [
                "## (Demo) Load TSLA series + explore\n",
                "\n",
                "This section demonstrates the typical first steps: load a single ticker time series, check coverage and missingness, and visualize prices/returns.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "c42a4441",
            "metadata": {},
            "outputs": [],
            "source": [
                "if TRAIN_SPLIT_PATH.exists() and TEST_SPLIT_PATH.exists():\n",
                "    tsla_train = pd.read_parquet(TRAIN_SPLIT_PATH).copy()\n",
                "    tsla_test  = pd.read_parquet(TEST_SPLIT_PATH).copy()\n",
                "\n",
                "    for df in (tsla_train, tsla_test):\n",
                "        df['date'] = pd.to_datetime(df['date'])\n",
                "        df.sort_values('date', inplace=True)\n",
                "\n",
                "    tsla_full = (\n",
                "        pd.concat([tsla_train, tsla_test], ignore_index=True)\n",
                "          .sort_values('date')\n",
                "          .drop_duplicates(subset=['date'])\n",
                "          .reset_index(drop=True)\n",
                "    )\n",
                "\n",
                "    print('Train:', tsla_train.shape, tsla_train['date'].min(), '→', tsla_train['date'].max())\n",
                "    print('Test: ', tsla_test.shape,  tsla_test['date'].min(),  '→', tsla_test['date'].max())\n",
                "    print('Full: ', tsla_full.shape,  tsla_full['date'].min(),  '→', tsla_full['date'].max())\n",
                "\n",
                "    display(tsla_full.head())\n",
                "    display(tsla_full.tail())\n",
                "    print('Missing values:\\n', tsla_full.isna().sum())\n",
                "\n",
                "    plt.figure(figsize=(12, 4))\n",
                "    plt.plot(tsla_full['date'], tsla_full['adj_close'], linewidth=1.5)\n",
                "    plt.title('TSLA adjusted close (train + test)')\n",
                "    plt.xlabel('Date')\n",
                "    plt.ylabel('Adj Close')\n",
                "    plt.grid(True, alpha=0.3)\n",
                "    plt.tight_layout()\n",
                "    plt.show()\n",
                "\n",
                "    # log returns\n",
                "    tsla_full['log_ret'] = np.log(tsla_full['adj_close']).diff()\n",
                "\n",
                "    plt.figure(figsize=(12, 3))\n",
                "    plt.plot(tsla_full['date'], tsla_full['log_ret'], linewidth=1)\n",
                "    plt.title('TSLA log returns')\n",
                "    plt.xlabel('Date')\n",
                "    plt.ylabel('log return')\n",
                "    plt.grid(True, alpha=0.3)\n",
                "    plt.tight_layout()\n",
                "    plt.show()\n",
                "\n",
                "    display(tsla_full['log_ret'].describe())\n",
                "else:\n",
                "    print('Skipping underlying-series EDA: train/test parquet files not found.')\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "def9ce0c",
            "metadata": {},
            "source": [
                "## (Demo) Test stationarity using the ADF test\n",
                "\n",
                "Typically, price levels are non-stationary, while returns are closer to stationary.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "cfd346e2",
            "metadata": {},
            "outputs": [],
            "source": [
                "if 'tsla_full' in globals():\n",
                "    from typing import Mapping, Tuple, cast\n",
                "\n",
                "    from statsmodels.tsa.stattools import adfuller\n",
                "\n",
                "    def adf_report(series, name):\n",
                "        series = pd.Series(series).dropna()\n",
                "\n",
                "        # `adfuller` has multiple return shapes depending on args.\n",
                "        # We always call it with autolag='AIC' (=> includes `icbest`) and default `store=False`.\n",
                "        ADFResult = Tuple[float, float, int, int, Mapping[str, float], float]\n",
                "        result = cast(ADFResult, adfuller(series, autolag='AIC'))\n",
                "\n",
                "        stat, pvalue, usedlag, nobs, crit, icbest = result\n",
                "\n",
                "        print(f'ADF test — {name}')\n",
                "        print(f'  test statistic: {stat:.4f}')\n",
                "        print(f'  p-value:        {pvalue:.6f}')\n",
                "        print(f'  lags used:      {usedlag}')\n",
                "        print(f'  nobs:           {nobs}')\n",
                "        print('  critical values:', {k: float(v) for k, v in crit.items()})\n",
                "        print(f'  icbest:         {icbest:.4f}')\n",
                "        print('  conclusion:', 'stationary (reject unit root)' if pvalue < 0.05 else 'non-stationary (fail to reject)')\n",
                "        print()\n",
                "\n",
                "    adf_report(tsla_full['adj_close'], 'adj_close (price level)')\n",
                "    adf_report(tsla_full['log_ret'], 'log returns')\n",
                "else:\n",
                "    print('Skipping ADF test: underlying series not loaded.')\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "95c924e4",
            "metadata": {},
            "source": [
                "## 2) Load forecasts (aligned to test dates)\n",
                "\n",
                "We load:\n",
                "- ARIMA forecast CSV\n",
                "- LSTM forecast CSV\n",
                "- Merged forecast CSV (should align by test dates)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "d0b99d07",
            "metadata": {},
            "outputs": [],
            "source": [
                "arima_fc = pd.read_csv(ARIMA_FORECAST_PATH)\n",
                "lstm_fc = pd.read_csv(LSTM_FORECAST_PATH)\n",
                "merged_fc = pd.read_csv(MERGED_FORECASTS_PATH)\n",
                "\n",
                "for df in (arima_fc, lstm_fc, merged_fc):\n",
                "    df['date'] = pd.to_datetime(df['date'])\n",
                "\n",
                "print('ARIMA forecast shape:', arima_fc.shape)\n",
                "print('LSTM forecast shape:', lstm_fc.shape)\n",
                "print('Merged forecast shape:', merged_fc.shape)\n",
                "\n",
                "display(arima_fc.head())\n",
                "display(lstm_fc.head())\n",
                "display(merged_fc.head())\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "655f72a0",
            "metadata": {},
            "source": [
                "### Forecast alignment checks\n",
                "We verify that:\n",
                "- test dates are sorted\n",
                "- merged dates match ARIMA dates\n",
                "- merged contains both model predictions\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "d4342d6f",
            "metadata": {},
            "outputs": [],
            "source": [
                "def assert_monotonic_dates(df, name='df'):\n",
                "    if not df['date'].is_monotonic_increasing:\n",
                "        raise ValueError(f'{name} dates are not sorted ascending')\n",
                "\n",
                "assert_monotonic_dates(arima_fc, 'arima_fc')\n",
                "assert_monotonic_dates(lstm_fc, 'lstm_fc')\n",
                "assert_monotonic_dates(merged_fc, 'merged_fc')\n",
                "\n",
                "dates_arima = set(arima_fc['date'])\n",
                "dates_merged = set(merged_fc['date'])\n",
                "\n",
                "print('Dates in ARIMA but not merged:', len(dates_arima - dates_merged))\n",
                "print('Dates in merged but not ARIMA:', len(dates_merged - dates_arima))\n",
                "\n",
                "missing_cols = [c for c in ['y_true', 'arima_pred', 'lstm_pred'] if c not in merged_fc.columns]\n",
                "if missing_cols:\n",
                "    raise ValueError(f'Merged forecasts missing columns: {missing_cols}')\n",
                "\n",
                "print('Merged columns OK.')\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "5027b28a",
            "metadata": {},
            "source": [
                "## 3) Load model specifications (ARIMA/SARIMA parameters + LSTM architecture)\n",
                "\n",
                "These JSON files are the **documentation artifacts** required for the report.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "9a9f259d",
            "metadata": {},
            "outputs": [],
            "source": [
                "with open(ARIMA_PARAMS_PATH, 'r', encoding='utf-8') as f:\n",
                "    arima_params = json.load(f)\n",
                "\n",
                "with open(LSTM_ARCH_PATH, 'r', encoding='utf-8') as f:\n",
                "    lstm_info = json.load(f)\n",
                "\n",
                "display(arima_params)\n",
                "display(lstm_info)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "a04e6c1a",
            "metadata": {},
            "source": [
                "### ARIMA/SARIMA summary\n",
                "Key items to report:\n",
                "- `order = (p,d,q)`\n",
                "- `seasonal_order = (P,D,Q,m)` if seasonal\n",
                "- selection criterion (AIC)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "e840f233",
            "metadata": {},
            "outputs": [],
            "source": [
                "arima_summary = {\n",
                "    'asset': arima_params.get('asset'),\n",
                "    'target_col': arima_params.get('target_col'),\n",
                "    'seasonal': arima_params.get('seasonal'),\n",
                "    'm': arima_params.get('m'),\n",
                "    'order': arima_params.get('order'),\n",
                "    'seasonal_order': arima_params.get('seasonal_order'),\n",
                "    'aic': arima_params.get('aic'),\n",
                "    'bic': arima_params.get('bic'),\n",
                "}\n",
                "\n",
                "display(pd.DataFrame([arima_summary]))\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "3278718b",
            "metadata": {},
            "source": [
                "### LSTM architecture & training summary\n",
                "We report:\n",
                "- lookback window\n",
                "- features used\n",
                "- scaling strategy\n",
                "- layers/units/dropout\n",
                "- epochs/batch size/learning rate\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "b4fa2f68",
            "metadata": {},
            "outputs": [],
            "source": [
                "run_cfg = lstm_info.get('run', {})\n",
                "lstm_summary = {\n",
                "    'lookback': run_cfg.get('lookback'),\n",
                "    'horizon': run_cfg.get('horizon'),\n",
                "    'n_features': len(run_cfg.get('feature_cols', [])),\n",
                "    'feature_cols': ', '.join(run_cfg.get('feature_cols', [])),\n",
                "    'scaler_type': run_cfg.get('scaler_type'),\n",
                "    'units_1': run_cfg.get('units_1'),\n",
                "    'units_2': run_cfg.get('units_2'),\n",
                "    'dropout': run_cfg.get('dropout'),\n",
                "    'rec_dropout': run_cfg.get('rec_dropout'),\n",
                "    'epochs': run_cfg.get('epochs'),\n",
                "    'batch_size': run_cfg.get('batch_size'),\n",
                "    'learning_rate': run_cfg.get('learning_rate'),\n",
                "}\n",
                "\n",
                "display(pd.DataFrame([lstm_summary]).T.rename(columns={0:'value'}))\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "6f4ea2e9",
            "metadata": {},
            "source": [
                "## (Demo) Build an ARIMA model on returns (stationary data) and forecast prices\n",
                "\n",
                "This section demonstrates the classical workflow: model *returns* (approximately stationary) and map back to prices.\n",
                "\n",
                "> This is **illustrative** only. The **official** ARIMA/SARIMA results used for scoring are the script-generated artifacts loaded above.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "b75939e8",
            "metadata": {},
            "outputs": [],
            "source": [
                "if 'tsla_full' in globals():\n",
                "    import statsmodels.api as sm\n",
                "\n",
                "    demo_train_end = pd.to_datetime(split_info['cutoff_date'])\n",
                "    demo_train = tsla_full[tsla_full['date'] <= demo_train_end].copy()\n",
                "    demo_test  = tsla_full[tsla_full['date'] >  demo_train_end].copy()\n",
                "\n",
                "    # Stationary target: log returns\n",
                "    r_train = demo_train['log_ret'].dropna()\n",
                "\n",
                "    # Small ARIMA order as a demonstration\n",
                "    demo_order = (1, 0, 1)\n",
                "    model = sm.tsa.ARIMA(r_train, order=demo_order)\n",
                "    res = model.fit()\n",
                "\n",
                "    steps = len(demo_test)\n",
                "    r_fc = res.forecast(steps=steps)\n",
                "    r_fc.index = demo_test['date'].values\n",
                "\n",
                "    last_train_price = float(demo_train['adj_close'].iloc[-1])\n",
                "    price_fc = last_train_price * np.exp(np.cumsum(r_fc.values))\n",
                "\n",
                "    demo_arima_df = pd.DataFrame({\n",
                "        'date': demo_test['date'].values,\n",
                "        'actual_price': demo_test['adj_close'].values,\n",
                "        'demo_arima_price_fc': price_fc\n",
                "    })\n",
                "\n",
                "    display(demo_arima_df.head())\n",
                "\n",
                "    plt.figure(figsize=(12,4))\n",
                "    plt.plot(demo_arima_df['date'], demo_arima_df['actual_price'], label='Actual', linewidth=2)\n",
                "    plt.plot(demo_arima_df['date'], demo_arima_df['demo_arima_price_fc'], label=f'Demo ARIMA on returns {demo_order}', linewidth=1.5)\n",
                "    plt.title('Demo: ARIMA on log returns → implied price forecast (test period)')\n",
                "    plt.xlabel('Date'); plt.ylabel('Price')\n",
                "    plt.grid(True, alpha=0.3)\n",
                "    plt.legend()\n",
                "    plt.tight_layout()\n",
                "    plt.show()\n",
                "else:\n",
                "    print('Skipping ARIMA-on-returns demo: underlying series not loaded.')\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "10fea664",
            "metadata": {},
            "source": [
                "## (Demo) Prepare data for LSTM (scaling + sequence creation)\n",
                "\n",
                "This section demonstrates the LSTM-specific prep steps. The actual model training is done in scripts.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "7caf5362",
            "metadata": {},
            "outputs": [],
            "source": [
                "if FEATURES_TRAIN_PATH.exists() and FEATURES_TEST_PATH.exists():\n",
                "    feat_train = pd.read_parquet(FEATURES_TRAIN_PATH).copy()\n",
                "    feat_test  = pd.read_parquet(FEATURES_TEST_PATH).copy()\n",
                "\n",
                "    for df in (feat_train, feat_test):\n",
                "        df['date'] = pd.to_datetime(df['date'])\n",
                "        df.sort_values('date', inplace=True)\n",
                "\n",
                "    feature_cols = run_cfg.get('feature_cols', [])\n",
                "    lookback = int(run_cfg.get('lookback', 30))\n",
                "\n",
                "    X_train_df = feat_train[feature_cols].copy()\n",
                "    X_test_df  = feat_test[feature_cols].copy()\n",
                "\n",
                "    from sklearn.preprocessing import MinMaxScaler\n",
                "    scaler_X = MinMaxScaler()\n",
                "    X_train_scaled = scaler_X.fit_transform(X_train_df.values)\n",
                "    X_test_scaled  = scaler_X.transform(X_test_df.values)\n",
                "\n",
                "    def make_sequences(X2d, lookback):\n",
                "        X_seq = []\n",
                "        for i in range(lookback, len(X2d)):\n",
                "            X_seq.append(X2d[i - lookback:i, :])\n",
                "        return np.array(X_seq)\n",
                "\n",
                "    X_train_seq = make_sequences(X_train_scaled, lookback)\n",
                "    X_test_seq  = make_sequences(X_test_scaled, lookback)\n",
                "\n",
                "    print('Feature columns:', feature_cols)\n",
                "    print('lookback:', lookback)\n",
                "    print('X_train_scaled:', X_train_scaled.shape)\n",
                "    print('X_train_seq:   ', X_train_seq.shape, '(samples, timesteps, features)')\n",
                "    print('X_test_seq:    ', X_test_seq.shape)\n",
                "\n",
                "    if 'adj_close' in feature_cols:\n",
                "        adj_idx = feature_cols.index('adj_close')\n",
                "        y_train_demo = X_train_scaled[lookback:, adj_idx]\n",
                "        y_test_demo  = X_test_scaled[lookback:, adj_idx]\n",
                "        print('y_train_demo:', y_train_demo.shape)\n",
                "        print('y_test_demo: ', y_test_demo.shape)\n",
                "    else:\n",
                "        print('Note: adj_close not found in feature_cols; check lstm_architecture.json')\n",
                "else:\n",
                "    print('Skipping LSTM prep demo: feature parquet files not found.')\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "53228124",
            "metadata": {},
            "source": [
                "## 4) Required metrics table (MAE / RMSE / MAPE)\n",
                "\n",
                "This is the required deliverable: **`model_comparison.csv`**.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "93fb5909",
            "metadata": {},
            "outputs": [],
            "source": [
                "comparison = pd.read_csv(MODEL_COMPARISON_PATH)\n",
                "display(comparison)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "53981117",
            "metadata": {},
            "source": [
                "### Sanity check: recompute metrics from merged forecasts\n",
                "We recompute MAE/RMSE/MAPE directly from `tsla_forecasts_merged.csv` to confirm consistency.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "87b1ff53",
            "metadata": {},
            "outputs": [],
            "source": [
                "def mae(y_true, y_pred):\n",
                "    y_true = np.asarray(y_true, dtype=float)\n",
                "    y_pred = np.asarray(y_pred, dtype=float)\n",
                "    return float(np.mean(np.abs(y_true - y_pred)))\n",
                "\n",
                "def rmse(y_true, y_pred):\n",
                "    y_true = np.asarray(y_true, dtype=float)\n",
                "    y_pred = np.asarray(y_pred, dtype=float)\n",
                "    return float(np.sqrt(np.mean((y_true - y_pred) ** 2)))\n",
                "\n",
                "def mape(y_true, y_pred, eps=1e-8):\n",
                "    y_true = np.asarray(y_true, dtype=float)\n",
                "    y_pred = np.asarray(y_pred, dtype=float)\n",
                "    denom = np.maximum(np.abs(y_true), eps)\n",
                "    return float(np.mean(np.abs((y_true - y_pred) / denom)) * 100.0)\n",
                "\n",
                "y_true = merged_fc['y_true'].values\n",
                "arima_pred = merged_fc['arima_pred'].values\n",
                "lstm_pred = merged_fc['lstm_pred'].values\n",
                "\n",
                "recalc = pd.DataFrame([\n",
                "    {'model': 'ARIMA_recalc', 'MAE': mae(y_true, arima_pred), 'RMSE': rmse(y_true, arima_pred), 'MAPE_pct': mape(y_true, arima_pred)},\n",
                "    {'model': 'LSTM_recalc',  'MAE': mae(y_true, lstm_pred),  'RMSE': rmse(y_true, lstm_pred),  'MAPE_pct': mape(y_true, lstm_pred)},\n",
                "])\n",
                "\n",
                "display(recalc)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "67e96e60",
            "metadata": {},
            "source": [
                "## 5) Visual comparison: Actual vs Forecasts on test period\n",
                "\n",
                "We display the saved figure if it exists; otherwise we generate the plot inline.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "9ce2fdc9",
            "metadata": {},
            "outputs": [],
            "source": [
                "if os.path.exists(FORECAST_PLOT_PATH):\n",
                "    from PIL import Image\n",
                "    img = Image.open(FORECAST_PLOT_PATH)\n",
                "    display(img)\n",
                "else:\n",
                "    print('Saved plot not found; generating inline plot...')\n",
                "    df = merged_fc.sort_values('date')\n",
                "\n",
                "    plt.figure(figsize=(12, 5))\n",
                "    plt.plot(df['date'], df['y_true'], label='Actual (TSLA adj_close)', linewidth=2)\n",
                "    plt.plot(df['date'], df['arima_pred'], label='ARIMA', linewidth=1.5)\n",
                "    plt.plot(df['date'], df['lstm_pred'], label='LSTM (multivariate)', linewidth=1.5)\n",
                "    plt.title('TSLA Forecasts on Test Period')\n",
                "    plt.xlabel('Date')\n",
                "    plt.ylabel('Price')\n",
                "    plt.grid(True, alpha=0.3)\n",
                "    plt.legend()\n",
                "    plt.tight_layout()\n",
                "    plt.show()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "82a7ed2d",
            "metadata": {},
            "source": [
                "## 6) Extra section — deliverable proof (head/tail) + error distribution diagnostics\n",
                "\n",
                "This section strengthens the report by:\n",
                "- Showing first/last rows of forecast artifacts (alignment proof)\n",
                "- Providing error distribution statistics (bias, dispersion, quantiles)\n",
                "- Showing how often one model beats the other per-day (absolute error)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "b98fe9fe",
            "metadata": {},
            "outputs": [],
            "source": [
                "def preview_df(df, name, n=5):\n",
                "    print(f'\\n{name} — head({n})')\n",
                "    display(df.head(n))\n",
                "    print(f'\\n{name} — tail({n})')\n",
                "    display(df.tail(n))\n",
                "\n",
                "preview_df(arima_fc, 'ARIMA forecast CSV')\n",
                "preview_df(lstm_fc, 'LSTM forecast CSV')\n",
                "preview_df(merged_fc, 'Merged forecasts CSV')\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "893383ad",
            "metadata": {},
            "source": [
                "### 6.2 Additional error diagnostics (per model) + win-rate\n",
                "\n",
                "- loads the saved diagnostics (error_diagnostics.csv) as the report source-of-truth\n",
                "- recomputes win-rate from merged_fc (optional) and checks if it matches the artifact\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "c99f7f89",
            "metadata": {},
            "outputs": [],
            "source": [
                "diag_script = pd.read_csv(ERROR_DIAGNOSTICS_PATH)\n",
                "\n",
                "diag_models = diag_script[diag_script['model'].isin(['ARIMA', 'LSTM_multivariate'])].copy()\n",
                "display(diag_models)\n",
                "\n",
                "df = merged_fc.sort_values('date').copy()\n",
                "comp = df[['date', 'y_true', 'arima_pred', 'lstm_pred']].dropna().copy()\n",
                "\n",
                "comp['abs_err_arima'] = (comp['arima_pred'] - comp['y_true']).abs()\n",
                "comp['abs_err_lstm']  = (comp['lstm_pred']  - comp['y_true']).abs()\n",
                "\n",
                "n = len(comp)\n",
                "lstm_win_rate_pct = (comp['abs_err_lstm'] < comp['abs_err_arima']).mean() * 100\n",
                "arima_win_rate_pct = (comp['abs_err_arima'] < comp['abs_err_lstm']).mean() * 100\n",
                "tie_rate_pct = (comp['abs_err_arima'] == comp['abs_err_lstm']).mean() * 100\n",
                "\n",
                "win_check = pd.DataFrame(\n",
                "    {\n",
                "        'win_rate_vs_other_pct_recalc': [arima_win_rate_pct, lstm_win_rate_pct, tie_rate_pct],\n",
                "        'n_compared': [n, n, n],\n",
                "    },\n",
                "    index=['ARIMA', 'LSTM_multivariate', 'TIES']\n",
                ")\n",
                "\n",
                "display(win_check)\n",
                "\n",
                "merged_check = diag_script[['model', 'win_rate_vs_other_pct']].merge(\n",
                "    win_check.reset_index().rename(columns={'index': 'model'}),\n",
                "    on='model',\n",
                "    how='inner'\n",
                ")\n",
                "\n",
                "merged_check['abs_diff_pct_points'] = (\n",
                "    merged_check['win_rate_vs_other_pct'] - merged_check['win_rate_vs_other_pct_recalc']\n",
                ").abs()\n",
                "\n",
                "display(merged_check)\n",
                "\n",
                "max_diff = merged_check['abs_diff_pct_points'].max()\n",
                "print(f'Max absolute difference (pct points) between artifact and notebook recalc: {max_diff:.6f}')\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "1553ca53",
            "metadata": {},
            "source": [
                "### 6.4 Error over time (optional diagnostic plot)\n",
                "This plot helps identify whether one model drifts or fails during volatility regimes.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "864ad7b2",
            "metadata": {},
            "outputs": [],
            "source": [
                "plot_df = merged_fc.sort_values('date').copy()\n",
                "\n",
                "if 'err_arima' not in plot_df.columns or 'err_lstm' not in plot_df.columns:\n",
                "    y_col = 'y_true' if 'y_true' in plot_df.columns else ('actual' if 'actual' in plot_df.columns else None)\n",
                "    arima_col = 'arima_pred' if 'arima_pred' in plot_df.columns else ('arima' if 'arima' in plot_df.columns else None)\n",
                "    lstm_col = 'lstm_pred' if 'lstm_pred' in plot_df.columns else ('lstm' if 'lstm' in plot_df.columns else None)\n",
                "    if not (y_col and arima_col and lstm_col):\n",
                "        raise KeyError(\n",
                "            'Expected columns for error plotting not found. '\n",
                "            f'Need y_true/actual, arima_pred/arima, lstm_pred/lstm. Got: {list(plot_df.columns)}'\n",
                "        )\n",
                "    plot_df['err_arima'] = plot_df[arima_col] - plot_df[y_col]\n",
                "    plot_df['err_lstm'] = plot_df[lstm_col] - plot_df[y_col]\n",
                "\n",
                "plt.figure(figsize=(12, 4))\n",
                "plt.plot(plot_df['date'], plot_df['err_arima'], label='ARIMA error (pred-actual)', linewidth=1)\n",
                "plt.plot(plot_df['date'], plot_df['err_lstm'], label='LSTM error (pred-actual)', linewidth=1)\n",
                "plt.axhline(0, color='black', linewidth=1, alpha=0.7)\n",
                "plt.title('Forecast Errors Over Test Period')\n",
                "plt.xlabel('Date')\n",
                "plt.ylabel('Error')\n",
                "plt.grid(True, alpha=0.3)\n",
                "plt.legend()\n",
                "plt.tight_layout()\n",
                "plt.show()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "771fa707",
            "metadata": {},
            "source": [
                "## 7) Discussion (brief, metrics-grounded)\n",
                "\n",
                "### Model behavior considerations\n",
                "- **ARIMA/SARIMA** (univariate) models linear autocorrelation structure in `adj_close`. It tends to perform well when the series is relatively smooth and the next-step value is strongly related to recent values.\n",
                "- **Multivariate LSTM** can exploit nonlinear relationships and additional predictors (OHLCV and engineered indicators), but it is sensitive to feature scaling, lookback choice, and regime shifts/volatility spikes.\n",
                "\n",
                "### Which model is better here?\n",
                "We choose the model with **lower RMSE** (primary), then MAE and MAPE as supporting metrics.\n",
                "\n",
                "### Notes on validity / leakage\n",
                "- Chronological split is enforced by cutoff date.\n",
                "- LSTM scalers are fit on **train only** (per scripts) to avoid leakage.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "af5a0d15",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Auto-generate a short conclusion snippet based on RMSE\n",
                "cmp = comparison.copy().sort_values('RMSE')\n",
                "best = cmp.iloc[0].to_dict()\n",
                "runner_up = cmp.iloc[1].to_dict() if len(cmp) > 1 else None\n",
                "\n",
                "print('Best model by RMSE:')\n",
                "print(best)\n",
                "\n",
                "if runner_up:\n",
                "    print('\\nRunner-up:')\n",
                "    print(runner_up)\n",
                "\n",
                "print('\\nConclusion draft:')\n",
                "print(\n",
                "    f\"Based on the test-period evaluation, the best-performing model is {best['model']} \"\n",
                "    f\"with RMSE={best['RMSE']:.4f}, MAE={best['MAE']:.4f}, MAPE={best['MAPE_pct']:.2f}%. \"\n",
                "    'This indicates it better captures the short-horizon dynamics of TSLA adj_close over the held-out period.'\n",
                ")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "4540adeb",
            "metadata": {},
            "source": [
                "## 8) Deliverables checklist (for submission)\n",
                "\n",
                "Confirm these files exist in your repo after running scripts:\n",
                "\n",
                "- **Splits & features**\n",
                "  - `data/task2/splits/tsla_train.parquet`\n",
                "  - `data/task2/splits/tsla_test.parquet`\n",
                "  - `data/task2/features/tsla_features_train.parquet`\n",
                "  - `data/task2/features/tsla_features_test.parquet`\n",
                "\n",
                "- **Model artifacts**\n",
                "  - `outputs/task2/models/lstm_model.keras`\n",
                "  - (optional) `outputs/task2/models/arima_model.pkl`\n",
                "\n",
                "- **Forecasts (aligned to test dates)**\n",
                "  - `outputs/task2/forecasts/tsla_arima_forecast.csv`\n",
                "  - `outputs/task2/forecasts/tsla_lstm_forecast.csv`\n",
                "  - `outputs/task2/forecasts/tsla_forecasts_merged.csv`\n",
                "\n",
                "- **Metrics & documentation**\n",
                "  - `outputs/task2/metrics/model_comparison.csv`  ✅ required table\n",
                "  - `outputs/task2/metrics/arima_params.json`      ✅ ARIMA/SARIMA parameters\n",
                "  - `outputs/task2/metrics/lstm_architecture.json` ✅ LSTM architecture\n",
                "  - `outputs/task2/metrics/split_info.json`        ✅ cutoff proof\n",
                "\n",
                "- **Figures**\n",
                "  - `outputs/task2/figures/forecast_test_period.png`\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Menv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.13.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
