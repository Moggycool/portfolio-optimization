{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Task 2 — TSLA Time Series Forecasting (ARIMA/SARIMA vs Multivariate LSTM)\n",
                "\n",
                "This notebook is the **only** notebook for Task 2. All modeling and artifact generation is done via scripts.\n",
                "\n",
                "## Required deliverables shown here\n",
                "1. **Chronological split** with cutoff on the **last trading day of 2024** (loaded from `split_info.json`).\n",
                "2. **ARIMA/SARIMA model specification** (loaded from `arima_params.json`).\n",
                "3. **LSTM architecture + training configuration** (loaded from `lstm_architecture.json`).\n",
                "4. **Forecast CSVs aligned to test dates** (`tsla_arima_forecast.csv`, `tsla_lstm_forecast.csv`, `tsla_forecasts_merged.csv`).\n",
                "5. **Performance metrics table** with MAE/RMSE/MAPE (`model_comparison.csv`).\n",
                "6. A brief **discussion**: which model performed better and why, grounded in the metrics and behavior.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Standard imports\n",
                "import os\n",
                "import json\n",
                "import warnings\n",
                "from pathlib import Path\n",
                "\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "warnings.filterwarnings(\"ignore\")\n",
                "\n",
                "def _find_repo_root(start: Path) -> Path:\n",
                "    start = start.resolve()\n",
                "    for candidate in [start, *start.parents]:\n",
                "        if (candidate / \"src\").is_dir() and (candidate / \"outputs\").exists():\n",
                "            return candidate\n",
                "    return start\n",
                "\n",
                "# Ensure REPO_ROOT is always a concrete Path (avoids Path | Unbound type issues in VS Code/Pylance)\n",
                "_existing_repo_root = globals().get(\"REPO_ROOT\", None)\n",
                "if isinstance(_existing_repo_root, Path):\n",
                "    REPO_ROOT = _existing_repo_root\n",
                "else:\n",
                "    REPO_ROOT = _find_repo_root(Path.cwd())\n",
                "    globals()[\"REPO_ROOT\"] = REPO_ROOT\n",
                "    print(\"Repo root (auto-detected):\", REPO_ROOT)\n",
                "\n",
                "ERROR_DIAGNOSTICS_PATH = REPO_ROOT / \"outputs\" / \"task2\" / \"metrics\" / \"error_diagnostics.csv\"\n",
                "\n",
                "# Display settings\n",
                "pd.set_option(\"display.max_columns\", 200)\n",
                "pd.set_option(\"display.width\", 120)\n",
                "\n",
                "print(\"Notebook working directory:\", os.getcwd())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 0) Define paths (repo-relative)\n",
                "\n",
                "These paths match the script outputs defined in `src/config.py`.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Repo-relative paths (robust to running from notebooks/)\n",
                "from pathlib import Path\n",
                "\n",
                "def _find_repo_root(start: Path) -> Path:\n",
                "    start = start.resolve()\n",
                "    for candidate in [start, *start.parents]:\n",
                "        if (candidate / \"src\").is_dir() and (candidate / \"outputs\").exists():\n",
                "            return candidate\n",
                "    return start\n",
                "\n",
                "REPO_ROOT = _find_repo_root(Path.cwd())\n",
                "print(\"Repo root:\", REPO_ROOT)\n",
                "\n",
                "SPLIT_INFO_PATH = REPO_ROOT / \"outputs\" / \"task2\" / \"metrics\" / \"split_info.json\"\n",
                "ARIMA_PARAMS_PATH = REPO_ROOT / \"outputs\" / \"task2\" / \"metrics\" / \"arima_params.json\"\n",
                "LSTM_ARCH_PATH = REPO_ROOT / \"outputs\" / \"task2\" / \"metrics\" / \"lstm_architecture.json\"\n",
                "MODEL_COMPARISON_PATH = REPO_ROOT / \"outputs\" / \"task2\" / \"metrics\" / \"model_comparison.csv\"\n",
                "\n",
                "ARIMA_FORECAST_PATH = REPO_ROOT / \"outputs\" / \"task2\" / \"forecasts\" / \"tsla_arima_forecast.csv\"\n",
                "LSTM_FORECAST_PATH = REPO_ROOT / \"outputs\" / \"task2\" / \"forecasts\" / \"tsla_lstm_forecast.csv\"\n",
                "MERGED_FORECASTS_PATH = REPO_ROOT / \"outputs\" / \"task2\" / \"forecasts\" / \"tsla_forecasts_merged.csv\"\n",
                "\n",
                "FORECAST_PLOT_PATH = REPO_ROOT / \"outputs\" / \"task2\" / \"figures\" / \"forecast_test_period.png\"\n",
                "\n",
                "required_files = [\n",
                "    SPLIT_INFO_PATH,\n",
                "    ARIMA_PARAMS_PATH,\n",
                "    LSTM_ARCH_PATH,\n",
                "    MODEL_COMPARISON_PATH,\n",
                "    ARIMA_FORECAST_PATH,\n",
                "    LSTM_FORECAST_PATH,\n",
                "    MERGED_FORECASTS_PATH,\n",
                " ]\n",
                "\n",
                "missing = [p for p in required_files if not p.exists()]\n",
                "if missing:\n",
                "    print(\"Missing required artifacts. Run scripts first:\")\n",
                "    for m in missing:\n",
                "        print(\" -\", m)\n",
                "else:\n",
                "    print(\"All required artifacts exist.\")\n",
                "\n",
                "print(\"Optional plot exists:\", FORECAST_PLOT_PATH.exists())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1) Load split information (proof of correct chronological cutoff)\n",
                "\n",
                "Task requirement: chronological split where training ends on the **last trading day of 2024**.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "with SPLIT_INFO_PATH.open(\"r\", encoding=\"utf-8\") as f:\n",
                "    split_info = json.load(f)\n",
                "\n",
                "split_info"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Split summary\n",
                "- Asset: **TSLA**\n",
                "- Split year: **2024**\n",
                "- Cutoff date (train end): **last trading day in 2024**\n",
                "- Test period begins the next trading day and continues through the dataset end\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "split_summary = pd.DataFrame([split_info])\n",
                "split_summary"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2) Load forecasts (aligned to test dates)\n",
                "\n",
                "We load:\n",
                "- ARIMA forecast CSV\n",
                "- LSTM forecast CSV\n",
                "- Merged forecast CSV (should align by test dates)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "arima_fc = pd.read_csv(ARIMA_FORECAST_PATH)\n",
                "lstm_fc = pd.read_csv(LSTM_FORECAST_PATH)\n",
                "merged_fc = pd.read_csv(MERGED_FORECASTS_PATH)\n",
                "\n",
                "for df in (arima_fc, lstm_fc, merged_fc):\n",
                "    df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
                "\n",
                "print(\"ARIMA forecast shape:\", arima_fc.shape)\n",
                "print(\"LSTM forecast shape:\", lstm_fc.shape)\n",
                "print(\"Merged forecast shape:\", merged_fc.shape)\n",
                "\n",
                "arima_fc.head()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "lstm_fc.head()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "merged_fc.head()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Forecast alignment checks\n",
                "We verify that:\n",
                "- test dates are sorted\n",
                "- merged dates match ARIMA dates\n",
                "- merged contains both model predictions\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def assert_monotonic_dates(df, name=\"df\"):\n",
                "    if not df[\"date\"].is_monotonic_increasing:\n",
                "        raise ValueError(f\"{name} dates are not sorted ascending\")\n",
                "\n",
                "assert_monotonic_dates(arima_fc, \"arima_fc\")\n",
                "assert_monotonic_dates(lstm_fc, \"lstm_fc\")\n",
                "assert_monotonic_dates(merged_fc, \"merged_fc\")\n",
                "\n",
                "dates_arima = set(arima_fc[\"date\"])\n",
                "dates_merged = set(merged_fc[\"date\"])\n",
                "\n",
                "print(\"Dates in ARIMA but not merged:\", len(dates_arima - dates_merged))\n",
                "print(\"Dates in merged but not ARIMA:\", len(dates_merged - dates_arima))\n",
                "\n",
                "missing_cols = [c for c in [\"y_true\", \"arima_pred\", \"lstm_pred\"] if c not in merged_fc.columns]\n",
                "if missing_cols:\n",
                "    raise ValueError(f\"Merged forecasts missing columns: {missing_cols}\")\n",
                "\n",
                "print(\"Merged columns OK.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3) Load model specifications (ARIMA/SARIMA parameters + LSTM architecture)\n",
                "\n",
                "These JSON files are the **documentation artifacts** required for the report.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "with open(ARIMA_PARAMS_PATH, \"r\", encoding=\"utf-8\") as f:\n",
                "    arima_params = json.load(f)\n",
                "\n",
                "with open(LSTM_ARCH_PATH, \"r\", encoding=\"utf-8\") as f:\n",
                "    lstm_info = json.load(f)\n",
                "\n",
                "arima_params"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### ARIMA/SARIMA summary\n",
                "Key items to report:\n",
                "- `order = (p,d,q)`\n",
                "- `seasonal_order = (P,D,Q,m)` if seasonal\n",
                "- selection criterion (AIC)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "arima_summary = {\n",
                "    \"asset\": arima_params.get(\"asset\"),\n",
                "    \"target_col\": arima_params.get(\"target_col\"),\n",
                "    \"seasonal\": arima_params.get(\"seasonal\"),\n",
                "    \"m\": arima_params.get(\"m\"),\n",
                "    \"order\": arima_params.get(\"order\"),\n",
                "    \"seasonal_order\": arima_params.get(\"seasonal_order\"),\n",
                "    \"aic\": arima_params.get(\"aic\"),\n",
                "    \"bic\": arima_params.get(\"bic\"),\n",
                "}\n",
                "pd.DataFrame([arima_summary])"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### LSTM architecture & training summary\n",
                "We report:\n",
                "- lookback window\n",
                "- features used\n",
                "- scaling strategy\n",
                "- layers/units/dropout\n",
                "- epochs/batch size/learning rate\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "lstm_info"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "run_cfg = lstm_info.get(\"run\", {})\n",
                "lstm_summary = {\n",
                "    \"lookback\": run_cfg.get(\"lookback\"),\n",
                "    \"horizon\": run_cfg.get(\"horizon\"),\n",
                "    \"n_features\": len(run_cfg.get(\"feature_cols\", [])),\n",
                "    \"feature_cols\": \", \".join(run_cfg.get(\"feature_cols\", [])),\n",
                "    \"scaler_type\": run_cfg.get(\"scaler_type\"),\n",
                "    \"units_1\": run_cfg.get(\"units_1\"),\n",
                "    \"units_2\": run_cfg.get(\"units_2\"),\n",
                "    \"dropout\": run_cfg.get(\"dropout\"),\n",
                "    \"rec_dropout\": run_cfg.get(\"rec_dropout\"),\n",
                "    \"epochs\": run_cfg.get(\"epochs\"),\n",
                "    \"batch_size\": run_cfg.get(\"batch_size\"),\n",
                "    \"learning_rate\": run_cfg.get(\"learning_rate\"),\n",
                "}\n",
                "pd.DataFrame([lstm_summary]).T.rename(columns={0:\"value\"})"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4) Required metrics table (MAE / RMSE / MAPE)\n",
                "\n",
                "This is the required deliverable: **`model_comparison.csv`**.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "comparison = pd.read_csv(MODEL_COMPARISON_PATH)\n",
                "comparison"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Sanity check: recompute metrics from merged forecasts\n",
                "We recompute MAE/RMSE/MAPE directly from `tsla_forecasts_merged.csv` to confirm consistency.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def mae(y_true, y_pred):\n",
                "    y_true = np.asarray(y_true, dtype=float)\n",
                "    y_pred = np.asarray(y_pred, dtype=float)\n",
                "    return float(np.mean(np.abs(y_true - y_pred)))\n",
                "\n",
                "def rmse(y_true, y_pred):\n",
                "    y_true = np.asarray(y_true, dtype=float)\n",
                "    y_pred = np.asarray(y_pred, dtype=float)\n",
                "    return float(np.sqrt(np.mean((y_true - y_pred) ** 2)))\n",
                "\n",
                "def mape(y_true, y_pred, eps=1e-8):\n",
                "    y_true = np.asarray(y_true, dtype=float)\n",
                "    y_pred = np.asarray(y_pred, dtype=float)\n",
                "    denom = np.maximum(np.abs(y_true), eps)\n",
                "    return float(np.mean(np.abs((y_true - y_pred) / denom)) * 100.0)\n",
                "\n",
                "y_true = merged_fc[\"y_true\"].values\n",
                "arima_pred = merged_fc[\"arima_pred\"].values\n",
                "lstm_pred = merged_fc[\"lstm_pred\"].values\n",
                "\n",
                "recalc = pd.DataFrame([\n",
                "    {\"model\": \"ARIMA_recalc\", \"MAE\": mae(y_true, arima_pred), \"RMSE\": rmse(y_true, arima_pred), \"MAPE_pct\": mape(y_true, arima_pred)},\n",
                "    {\"model\": \"LSTM_recalc\", \"MAE\": mae(y_true, lstm_pred), \"RMSE\": rmse(y_true, lstm_pred), \"MAPE_pct\": mape(y_true, lstm_pred)},\n",
                "])\n",
                "recalc"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5) Visual comparison: Actual vs Forecasts on test period\n",
                "\n",
                "We display the saved figure if it exists; otherwise we generate the plot inline.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Show saved plot if present\n",
                "if os.path.exists(FORECAST_PLOT_PATH):\n",
                "    from PIL import Image\n",
                "    img = Image.open(FORECAST_PLOT_PATH)\n",
                "    display(img)\n",
                "else:\n",
                "    print(\"Saved plot not found; generating inline plot...\")\n",
                "    df = merged_fc.sort_values(\"date\")\n",
                "\n",
                "    plt.figure(figsize=(12, 5))\n",
                "    plt.plot(df[\"date\"], df[\"y_true\"], label=\"Actual (TSLA adj_close)\", linewidth=2)\n",
                "    plt.plot(df[\"date\"], df[\"arima_pred\"], label=\"ARIMA\", linewidth=1.5)\n",
                "    plt.plot(df[\"date\"], df[\"lstm_pred\"], label=\"LSTM (multivariate)\", linewidth=1.5)\n",
                "    plt.title(\"TSLA Forecasts on Test Period\")\n",
                "    plt.xlabel(\"Date\")\n",
                "    plt.ylabel(\"Price\")\n",
                "    plt.grid(True, alpha=0.3)\n",
                "    plt.legend()\n",
                "    plt.tight_layout()\n",
                "    plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6) Extra section — deliverable proof (head/tail) + error distribution diagnostics\n",
                "\n",
                "This section strengthens the report by:\n",
                "- Showing **first/last rows** of forecast artifacts (alignment proof)\n",
                "- Providing error distribution statistics (bias, dispersion, quantiles)\n",
                "- Showing how often one model beats the other per-day (absolute error)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 6.1 Forecast artifact previews (head / tail)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def preview_df(df, name, n=5):\n",
                "    print(f\"\\n{name} — head({n})\")\n",
                "    display(df.head(n))\n",
                "    print(f\"\\n{name} — tail({n})\")\n",
                "    display(df.tail(n))\n",
                "\n",
                "preview_df(arima_fc, \"ARIMA forecast CSV\")\n",
                "preview_df(lstm_fc, \"LSTM forecast CSV\")\n",
                "preview_df(merged_fc, \"Merged forecasts CSV\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 6.2 Additional error diagnostics (per model)\n",
                "\n",
                "This block:\n",
                "\n",
                "- loads the saved diagnostics (error_diagnostics.csv) as the report source-of-truth\n",
                "- recomputes win-rate from merged_fc (optional) and checks if it matches the artifact\n",
                "- removes the confusing “two win-rate tables” situation\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "## 6.2 Additional error diagnostics (per model) + win-rate (from artifact + verified)\n",
                "\n",
                "# 1) Load script-generated diagnostics (source-of-truth for reporting)\n",
                "diag_script = pd.read_csv(ERROR_DIAGNOSTICS_PATH)\n",
                "\n",
                "# Keep only the real model rows for display (exclude the \"TIES\" summary row)\n",
                "diag_models = diag_script[diag_script[\"model\"].isin([\"ARIMA\", \"LSTM_multivariate\"])].copy()\n",
                "\n",
                "display(diag_models)\n",
                "\n",
                "# 2) Also compute win-rate from merged_fc to verify it matches the artifact\n",
                "df = merged_fc.sort_values(\"date\").copy()\n",
                "comp = df[[\"date\", \"y_true\", \"arima_pred\", \"lstm_pred\"]].dropna().copy()\n",
                "\n",
                "comp[\"abs_err_arima\"] = (comp[\"arima_pred\"] - comp[\"y_true\"]).abs()\n",
                "comp[\"abs_err_lstm\"]  = (comp[\"lstm_pred\"]  - comp[\"y_true\"]).abs()\n",
                "\n",
                "n = len(comp)\n",
                "lstm_win_rate_pct = (comp[\"abs_err_lstm\"] < comp[\"abs_err_arima\"]).mean() * 100\n",
                "arima_win_rate_pct = (comp[\"abs_err_arima\"] < comp[\"abs_err_lstm\"]).mean() * 100\n",
                "tie_rate_pct = (comp[\"abs_err_arima\"] == comp[\"abs_err_lstm\"]).mean() * 100\n",
                "\n",
                "win_check = pd.DataFrame(\n",
                "    {\n",
                "        \"win_rate_vs_other_pct_recalc\": [arima_win_rate_pct, lstm_win_rate_pct, tie_rate_pct],\n",
                "        \"n_compared\": [n, n, n],\n",
                "    },\n",
                "    index=[\"ARIMA\", \"LSTM_multivariate\", \"TIES\"]\n",
                ")\n",
                "\n",
                "display(win_check)\n",
                "\n",
                "# 3) Consistency check vs artifact\n",
                "# (Artifact includes a TIES row, so we can compare all three.)\n",
                "merged_check = diag_script[[\"model\", \"win_rate_vs_other_pct\"]].merge(\n",
                "    win_check.reset_index().rename(columns={\"index\": \"model\"}),\n",
                "    on=\"model\",\n",
                "    how=\"inner\"\n",
                ")\n",
                "\n",
                "merged_check[\"abs_diff_pct_points\"] = (\n",
                "    merged_check[\"win_rate_vs_other_pct\"] - merged_check[\"win_rate_vs_other_pct_recalc\"]\n",
                ").abs()\n",
                "\n",
                "display(merged_check)\n",
                "\n",
                "max_diff = merged_check[\"abs_diff_pct_points\"].max()\n",
                "print(f\"Max absolute difference (pct points) between artifact and notebook recalc: {max_diff:.6f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 6.4 Error over time (optional diagnostic plot)\n",
                "This plot helps identify whether one model drifts or fails during volatility regimes.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 6.4 Error over time (optional diagnostic plot)\n",
                "plot_df = merged_fc.sort_values(\"date\").copy()\n",
                "\n",
                "# Compute error columns if they aren't already present\n",
                "if \"err_arima\" not in plot_df.columns or \"err_lstm\" not in plot_df.columns:\n",
                "    y_col = \"y_true\" if \"y_true\" in plot_df.columns else (\"actual\" if \"actual\" in plot_df.columns else None)\n",
                "    arima_col = \"arima_pred\" if \"arima_pred\" in plot_df.columns else (\"arima\" if \"arima\" in plot_df.columns else None)\n",
                "    lstm_col = \"lstm_pred\" if \"lstm_pred\" in plot_df.columns else (\"lstm\" if \"lstm\" in plot_df.columns else None)\n",
                "    if not (y_col and arima_col and lstm_col):\n",
                "        raise KeyError(\n",
                "            \"Expected columns for error plotting not found. \"\n",
                "            f\"Need y_true/actual, arima_pred/arima, lstm_pred/lstm. Got: {list(plot_df.columns)}\"\n",
                "        )\n",
                "    plot_df[\"err_arima\"] = plot_df[arima_col] - plot_df[y_col]\n",
                "    plot_df[\"err_lstm\"] = plot_df[lstm_col] - plot_df[y_col]\n",
                "\n",
                "plt.figure(figsize=(12, 4))\n",
                "plt.plot(plot_df[\"date\"], plot_df[\"err_arima\"], label=\"ARIMA error (pred-actual)\", linewidth=1)\n",
                "plt.plot(plot_df[\"date\"], plot_df[\"err_lstm\"], label=\"LSTM error (pred-actual)\", linewidth=1)\n",
                "plt.axhline(0, color=\"black\", linewidth=1, alpha=0.7)\n",
                "plt.title(\"Forecast Errors Over Test Period\")\n",
                "plt.xlabel(\"Date\")\n",
                "plt.ylabel(\"Error\")\n",
                "plt.grid(True, alpha=0.3)\n",
                "plt.legend()\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7) Discussion (brief, metrics-grounded)\n",
                "\n",
                "### Model behavior considerations\n",
                "- **ARIMA/SARIMA** (univariate) models linear autocorrelation structure in `adj_close`. It tends to perform well when the series is relatively smooth and the next-step value is strongly related to recent values.\n",
                "- **Multivariate LSTM** can exploit nonlinear relationships and additional predictors (OHLCV and engineered indicators), but it is sensitive to:\n",
                "  - feature scaling,\n",
                "  - lookback choice,\n",
                "  - limited data regime / nonstationarity,\n",
                "  - regime shifts and volatility spikes.\n",
                "\n",
                "### Which model is better here?\n",
                "We choose the model with **lower RMSE** (primary), then MAE and MAPE as supporting metrics.\n",
                "\n",
                "Use the *Error diagnostics* section above to comment on:\n",
                "- bias (mean error): systematic over/under prediction\n",
                "- dispersion (std): stability of errors\n",
                "- quantiles: whether extreme errors are worse for one model\n",
                "\n",
                "### Notes on validity / leakage\n",
                "- Chronological split is enforced by cutoff date.\n",
                "- LSTM scalers are fit on **train only** (per scripts) to avoid leakage.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Auto-generate a short conclusion snippet based on RMSE\n",
                "cmp = comparison.copy()\n",
                "cmp = cmp.sort_values(\"RMSE\")\n",
                "best = cmp.iloc[0].to_dict()\n",
                "runner_up = cmp.iloc[1].to_dict() if len(cmp) > 1 else None\n",
                "\n",
                "print(\"Best model by RMSE:\")\n",
                "print(best)\n",
                "\n",
                "if runner_up:\n",
                "    print(\"\\nRunner-up:\")\n",
                "    print(runner_up)\n",
                "\n",
                "print(\"\\nConclusion draft:\")\n",
                "print(\n",
                "    f\"Based on the test-period evaluation, the best-performing model is {best['model']} \"\n",
                "    f\"with RMSE={best['RMSE']:.4f}, MAE={best['MAE']:.4f}, MAPE={best['MAPE_pct']:.2f}%. \"\n",
                "    \"This indicates it better captures the short-horizon dynamics of TSLA adj_close over the held-out period.\" \n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8) Deliverables checklist (for submission)\n",
                "\n",
                "Confirm these files exist in your repo after running scripts:\n",
                "\n",
                "- **Splits & features**\n",
                "  - `data/task2/splits/tsla_train.parquet`\n",
                "  - `data/task2/splits/tsla_test.parquet`\n",
                "  - `data/task2/features/tsla_features_train.parquet`\n",
                "  - `data/task2/features/tsla_features_test.parquet`\n",
                "\n",
                "- **Model artifacts**\n",
                "  - `outputs/task2/models/lstm_model.keras`\n",
                "  - (optional) `outputs/task2/models/arima_model.pkl`\n",
                "\n",
                "- **Forecasts (aligned to test dates)**\n",
                "  - `outputs/task2/forecasts/tsla_arima_forecast.csv`\n",
                "  - `outputs/task2/forecasts/tsla_lstm_forecast.csv`\n",
                "  - `outputs/task2/forecasts/tsla_forecasts_merged.csv`\n",
                "\n",
                "- **Metrics & documentation**\n",
                "  - `outputs/task2/metrics/model_comparison.csv`  ✅ required table\n",
                "  - `outputs/task2/metrics/arima_params.json`      ✅ ARIMA/SARIMA parameters\n",
                "  - `outputs/task2/metrics/lstm_architecture.json` ✅ LSTM architecture\n",
                "  - `outputs/task2/metrics/split_info.json`        ✅ cutoff proof\n",
                "\n",
                "- **Figures**\n",
                "  - `outputs/task2/figures/forecast_test_period.png`\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Menv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.13.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
