{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1 — Financial Analysis (Cleaning, Returns, Stationarity, Risk Metrics)\n",
    "\n",
    "This notebook is primarily a **reporting notebook**: the official deliverables are generated by scripts and saved to disk.\n",
    "\n",
    "## Rubric-fast deliverables covered here\n",
    "1. Scaling/normalization evidence (a saved scaled dataset)\n",
    "2. Three visualizations saved as files\n",
    "3. Returns + stationarity (ADF) + risk metrics loaded from artifacts\n",
    "\n",
    "**Artifacts expected (after running scripts):**\n",
    "- `data/task1/processed/prices.parquet`\n",
    "- `data/task1/processed/returns.parquet`\n",
    "- `data/task1/processed/scaled_task1_prices.parquet` ✅ scaling evidence\n",
    "- `data/task1/processed/task1_adf_results.csv`\n",
    "- `data/task1/processed/task1_risk_metrics.csv`\n",
    "- `outputs/task1/viz/task1_prices_timeseries.png` ✅ plot 1\n",
    "- `outputs/task1/viz/task1_daily_pct_change.png` ✅ plot 2\n",
    "- `outputs/task1/viz/task1_rolling_mean_std.png` ✅ plot 3\n",
    "- `data/task1/processed/task1_outliers.csv` ✅ Outlier evidence\n",
    "- `outputs/task1/viz/task1_outliers_returns.png` ✅ Outlier plot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb0592c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import importlib\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def _find_repo_root(start: Path) -> Path:\n",
    "    \"\"\"Walk upward until we find a folder containing both `src/` and `outputs/` or `data/`.\"\"\"\n",
    "    start = start.resolve()\n",
    "    for candidate in [start, *start.parents]:\n",
    "        if (candidate / \"src\").is_dir() and ((candidate / \"outputs\").exists() or (candidate / \"data\").exists()):\n",
    "            return candidate\n",
    "    return start\n",
    "\n",
    "\n",
    "REPO_ROOT = _find_repo_root(Path.cwd())\n",
    "\n",
    "# Make imports and relative paths work consistently from notebooks/\n",
    "if str(REPO_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(REPO_ROOT))\n",
    "os.chdir(REPO_ROOT)\n",
    "\n",
    "from src import config as config\n",
    "config = importlib.reload(config)\n",
    "\n",
    "print(\"Repo root:\", REPO_ROOT)\n",
    "print(\"Notebook working directory (after chdir):\", os.getcwd())\n",
    "print(\"config file:\", getattr(config, \"__file__\", None))\n",
    "print(\"PRICES_PATH:\", config.PRICES_PATH)\n",
    "print(\"RETURNS_PATH:\", config.RETURNS_PATH)\n",
    "print(\"TASK1_SCALED_PRICES_PATH:\", getattr(config, \"TASK1_SCALED_PRICES_PATH\", None))\n",
    "print(\"TASK1_VIZ_DIR:\", getattr(config, \"TASK1_VIZ_DIR\", None))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da51c9f7",
   "metadata": {},
   "source": [
    "## 1) Load Task 1 datasets (prices + returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d93fb27",
   "metadata": {},
   "outputs": [],
   "source": [
    "prices = pd.read_parquet(config.PRICES_PATH)\n",
    "returns = pd.read_parquet(config.RETURNS_PATH)\n",
    "\n",
    "display(prices.head())\n",
    "display(returns.head())\n",
    "print('prices shape:', prices.shape)\n",
    "print('returns shape:', returns.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82de0ef",
   "metadata": {},
   "source": [
    "## EDA visual evidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ea2cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1b) Quick EDA summary table (visible evidence)\n",
    "\n",
    "ret_col = \"return\" if \"return\" in returns.columns else None\n",
    "if ret_col:\n",
    "    summary = (returns.groupby(\"asset\")[ret_col]\n",
    "               .agg(count=\"count\", mean=\"mean\", std=\"std\", min=\"min\",\n",
    "                    q01=lambda s: s.quantile(0.01),\n",
    "                    q99=lambda s: s.quantile(0.99),\n",
    "                    max=\"max\")\n",
    "               .reset_index())\n",
    "    display(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8f25d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1c) Outlier Detection — explicit evidence (required)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Path: prefer config, fallback to repo convention\n",
    "outliers_path = getattr(config, \"TASK1_OUTLIERS_PATH\", None)\n",
    "if outliers_path is None:\n",
    "    outliers_path = str(REPO_ROOT / \"data\" / \"task1\" /\n",
    "                        \"processed\" / \"task1_outliers.csv\")\n",
    "\n",
    "if not os.path.exists(outliers_path):\n",
    "    raise FileNotFoundError(\n",
    "        f\"Missing outlier evidence: {outliers_path}\\n\"\n",
    "        \"Run the Task 1 scripts that generate task1_outliers.csv and commit it.\"\n",
    "    )\n",
    "\n",
    "outliers = pd.read_csv(outliers_path)\n",
    "outliers[\"date\"] = pd.to_datetime(outliers[\"date\"])\n",
    "\n",
    "print(\"Outliers file:\", outliers_path)\n",
    "print(\"outliers shape:\", outliers.shape)\n",
    "display(outliers.head(10))\n",
    "\n",
    "# Make the method explicit (match what your script used)\n",
    "Z_THR = 3.0  # set this to your script's threshold\n",
    "print(\n",
    "    f\"Outlier rule (explicit): flag daily returns where |zscore| >= {Z_THR} (computed per asset).\")\n",
    "\n",
    "# Summary counts\n",
    "counts = (outliers.assign(abs_z=lambda d: d[\"zscore\"].abs())\n",
    "          .groupby(\"asset\")\n",
    "          .agg(outlier_count=(\"zscore\", \"size\"),\n",
    "               max_abs_z=(\"zscore\", lambda s: float(np.max(np.abs(s)))))\n",
    "          .reset_index()\n",
    "          .sort_values(\"outlier_count\", ascending=False))\n",
    "\n",
    "display(counts)\n",
    "\n",
    "# Show the most extreme points (reviewer-friendly)\n",
    "display(outliers.assign(abs_z=outliers[\"zscore\"].abs())\n",
    "        .sort_values(\"abs_z\", ascending=False)\n",
    "        .head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fcb0464",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1d) Outlier visualization — highlight flagged points (required)\n",
    "\n",
    "outlier_fig = str(REPO_ROOT / \"outputs\" / \"task1\" / \"viz\" / \"task1_outliers_returns.png\")\n",
    "os.makedirs(os.path.dirname(outlier_fig), exist_ok=True)\n",
    "\n",
    "# Ensure returns has proper dtypes\n",
    "r = returns.copy()\n",
    "r[\"date\"] = pd.to_datetime(r[\"date\"])\n",
    "r[\"return\"] = r[\"return\"].astype(float)\n",
    "\n",
    "# Mark outliers by (date, asset) membership\n",
    "flag = outliers[[\"date\", \"asset\"]].drop_duplicates().assign(is_outlier=True)\n",
    "m = r.merge(flag, on=[\"date\", \"asset\"], how=\"left\")\n",
    "m[\"is_outlier\"] = m[\"is_outlier\"].fillna(False)\n",
    "\n",
    "assets = sorted(m[\"asset\"].unique())\n",
    "fig, axes = plt.subplots(len(assets), 1, figsize=(12, 3.2 * len(assets)), sharex=True)\n",
    "if len(assets) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for ax, a in zip(axes, assets):\n",
    "    g = m[m[\"asset\"] == a].sort_values(\"date\")\n",
    "    ax.plot(g[\"date\"], g[\"return\"], linewidth=1.0, label=f\"{a} returns\")\n",
    "    go = g[g[\"is_outlier\"]]\n",
    "    ax.scatter(go[\"date\"], go[\"return\"], s=25, color=\"red\", label=\"outliers\", zorder=3)\n",
    "    ax.axhline(0, color=\"black\", linewidth=1, alpha=0.6)\n",
    "    ax.set_title(f\"{a}: daily returns with outliers highlighted (z-score based)\")\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(outlier_fig, dpi=150)\n",
    "plt.close(fig)\n",
    "\n",
    "print(\"Saved outlier plot:\", outlier_fig)\n",
    "\n",
    "from IPython.display import Image, display\n",
    "display(Image(filename=outlier_fig))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04ea13e",
   "metadata": {},
   "source": [
    "## 2) Scaling / normalization evidence\n",
    "\n",
    "Rubric requirement: demonstrate scaling/normalization. Preferred evidence: a saved scaled dataset.\n",
    "\n",
    "This notebook will **auto-generate** a scaled dataset if it does not exist yet (see the next cell), and save it to `config.TASK1_SCALED_PRICES_PATH`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a286d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling/normalization evidence\n",
    "# If a scaled dataset exists, load it; otherwise generate it from `prices`.\n",
    "\n",
    "scaled_path = getattr(config, 'TASK1_SCALED_PRICES_PATH', None)\n",
    "if scaled_path is None:\n",
    "    # fallback to the repo convention used by this notebook\n",
    "    scaled_path = str(REPO_ROOT / 'data' / 'task1' / 'processed' / 'scaled_task1_prices.parquet')\n",
    "\n",
    "if os.path.exists(scaled_path):\n",
    "    scaled_prices = pd.read_parquet(scaled_path)\n",
    "    print('Loaded scaled dataset:', scaled_path)\n",
    "else:\n",
    "    if 'prices' not in globals():\n",
    "        prices = pd.read_parquet(config.PRICES_PATH)\n",
    "\n",
    "    price_col = getattr(config, 'PRICE_COL', 'adj_close')\n",
    "\n",
    "    def _minmax(s: pd.Series) -> pd.Series:\n",
    "        s = s.astype(float)\n",
    "        denom = (s.max() - s.min())\n",
    "        if denom == 0:\n",
    "            return s * 0.0\n",
    "        return (s - s.min()) / denom\n",
    "\n",
    "    scaled_prices = prices.copy()\n",
    "    scaled_prices[f'{price_col}_scaled'] = scaled_prices.groupby('asset')[price_col].transform(_minmax)\n",
    "\n",
    "    os.makedirs(os.path.dirname(scaled_path), exist_ok=True)\n",
    "    scaled_prices.to_parquet(scaled_path, index=False)\n",
    "    print('Created scaled dataset:', scaled_path)\n",
    "\n",
    "display(scaled_prices.head())\n",
    "print('scaled_prices shape:', scaled_prices.shape)\n",
    "print('scaled columns:', list(scaled_prices.columns))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8f782e",
   "metadata": {},
   "source": [
    "## 3) Visualizations (must exist as files)\n",
    "\n",
    "This section verifies the three plot files are present.\n",
    "\n",
    "If any are missing, rerun:\n",
    "```bash\n",
    "python scripts/02_task1_scale_and_viz.py\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7493fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "viz_dir = getattr(config, 'TASK1_VIZ_DIR', 'outputs/task1/viz')\n",
    "os.makedirs(viz_dir, exist_ok=True)\n",
    "\n",
    "expected = [\n",
    "    os.path.join(viz_dir, 'task1_prices_timeseries.png'),\n",
    "    os.path.join(viz_dir, 'task1_daily_pct_change.png'),\n",
    "    os.path.join(viz_dir, 'task1_rolling_mean_std.png'),\n",
    "]\n",
    "\n",
    "missing = [p for p in expected if not os.path.exists(p)]\n",
    "print('Expected plots:')\n",
    "for p in expected:\n",
    "    print(' -', p, 'OK' if os.path.exists(p) else 'MISSING')\n",
    "\n",
    "if missing:\n",
    "    print('\\nSome plots are missing; generating them now...')\n",
    "\n",
    "    if 'prices' not in globals():\n",
    "        prices = pd.read_parquet(config.PRICES_PATH)\n",
    "    if 'returns' not in globals():\n",
    "        returns = pd.read_parquet(config.RETURNS_PATH)\n",
    "\n",
    "    prices['date'] = pd.to_datetime(prices['date'])\n",
    "    returns['date'] = pd.to_datetime(returns['date'])\n",
    "\n",
    "    price_col = getattr(config, 'PRICE_COL', 'adj_close')\n",
    "\n",
    "    # 1) Prices time series\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    for asset, g in prices.sort_values('date').groupby('asset'):\n",
    "        plt.plot(g['date'], g[price_col], label=asset, linewidth=1.5)\n",
    "    plt.title(f'Prices over time ({price_col})')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel(price_col)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(expected[0], dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "    # 2) Daily percent change\n",
    "    tmp = None\n",
    "    if 'return' in returns.columns:\n",
    "        tmp = returns.copy()\n",
    "        tmp['pct_change'] = tmp['return'].astype(float) * 100.0\n",
    "    else:\n",
    "        tmp = prices.sort_values('date').copy()\n",
    "        tmp['pct_change'] = tmp.groupby('asset')[price_col].pct_change() * 100.0\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    for asset, g in tmp.groupby('asset'):\n",
    "        plt.plot(g['date'], g['pct_change'], label=asset, linewidth=1.0)\n",
    "    plt.title('Daily % change')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('%')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(expected[1], dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "    # 3) Rolling mean/std (20D) of daily % change\n",
    "    window = 20\n",
    "    tmp2 = tmp.sort_values('date').copy()\n",
    "    tmp2['roll_mean'] = tmp2.groupby('asset')['pct_change'].transform(lambda s: s.rolling(window).mean())\n",
    "    tmp2['roll_std'] = tmp2.groupby('asset')['pct_change'].transform(lambda s: s.rolling(window).std())\n",
    "\n",
    "    fig, axes = plt.subplots(2, 1, figsize=(12, 7), sharex=True)\n",
    "    for asset, g in tmp2.groupby('asset'):\n",
    "        axes[0].plot(g['date'], g['roll_mean'], label=asset, linewidth=1.2)\n",
    "        axes[1].plot(g['date'], g['roll_std'], label=asset, linewidth=1.2)\n",
    "\n",
    "    axes[0].set_title(f'Rolling mean of daily % change ({window}D)')\n",
    "    axes[1].set_title(f'Rolling std of daily % change ({window}D)')\n",
    "    for ax in axes:\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        ax.legend()\n",
    "\n",
    "    axes[1].set_xlabel('Date')\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(expected[2], dpi=150)\n",
    "    plt.close(fig)\n",
    "\n",
    "    missing = [p for p in expected if not os.path.exists(p)]\n",
    "\n",
    "if missing:\n",
    "    raise FileNotFoundError(\n",
    "        'Missing required Task 1 plot files:\\n' + '\\n'.join(missing)\n",
    "    )\n",
    "\n",
    "print('\\nAll required plot files exist.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b2a8b5",
   "metadata": {},
   "source": [
    "### Display the saved plot images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a931211",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3b) EDA — Visual evidence (required)\n",
    "\n",
    "from IPython.display import Image, display\n",
    "\n",
    "captions = {\n",
    "    expected[0]: \"Prices over time: shows overall trend/regime shifts and relative price levels across assets.\",\n",
    "    expected[1]: \"Daily % change: highlights volatility clustering and extreme move days (spikes).\",\n",
    "    expected[2]: \"Rolling mean/std: shows time-varying volatility and stability (or instability) of returns.\"\n",
    "}\n",
    "\n",
    "for p in expected:\n",
    "    display(Image(filename=p))\n",
    "    print(\"Figure:\", os.path.basename(p))\n",
    "    print(\"Interpretation:\", captions.get(p, \"\"))\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3eb5ff",
   "metadata": {},
   "source": [
    "## 4) Stationarity evidence (ADF results)\n",
    "\n",
    "ADF outputs should be saved to CSV by your Task 1 scripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401c9117",
   "metadata": {},
   "outputs": [],
   "source": [
    "adf_path = config.TASK1_ADF_PATH\n",
    "if not os.path.exists(adf_path):\n",
    "    raise FileNotFoundError(f'Missing ADF results: {adf_path}. Run Task 1 scripts.')\n",
    "\n",
    "adf = pd.read_csv(adf_path)\n",
    "display(adf)\n",
    "print('ADF file:', adf_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55606f4",
   "metadata": {},
   "source": [
    "## 5) Risk metrics evidence\n",
    "\n",
    "Risk metrics (e.g., annualized return/volatility, Sharpe, VaR) should be saved by scripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2b58ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "risk_path = config.TASK1_RISK_PATH\n",
    "if not os.path.exists(risk_path):\n",
    "    raise FileNotFoundError(f'Missing risk metrics: {risk_path}. Run Task 1 scripts.')\n",
    "\n",
    "risk = pd.read_csv(risk_path)\n",
    "display(risk)\n",
    "print('Risk metrics file:', risk_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) Quick interpretation (short)\n",
    "\n",
    "- Prices are typically non-stationary; returns/log returns tend to be closer to stationary.\n",
    "- Scaling provides comparable magnitudes across assets for visualization and certain models.\n",
    "- Risk metrics summarize reward vs risk and tail behavior for each asset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Menv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
