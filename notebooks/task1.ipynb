{
       "cells": [
              {
                     "cell_type": "markdown",
                     "id": "c8cae336",
                     "metadata": {},
                     "source": [
                            "# Task 1 — Financial Analysis (Cleaning, Returns, Stationarity, Risk Metrics)\n",
                            "\n",
                            "This notebook is primarily a **reporting notebook**: the official deliverables are generated by scripts and saved to disk.\n",
                            "\n",
                            "## Rubric-fast deliverables covered here\n",
                            "1. Scaling/normalization evidence (a saved scaled dataset)\n",
                            "2. Three visualizations saved as files\n",
                            "3. Returns + stationarity (ADF) + risk metrics loaded from artifacts\n",
                            "\n",
                            "**Artifacts expected (after running scripts):**\n",
                            "- `data/task1/processed/prices.parquet`\n",
                            "- `data/task1/processed/returns.parquet`\n",
                            "- `data/task1/processed/scaled_task1_prices.parquet` ✅ scaling evidence\n",
                            "- `data/task1/processed/task1_adf_results.csv`\n",
                            "- `data/task1/processed/task1_risk_metrics.csv`\n",
                            "- `outputs/task1/viz/task1_prices_timeseries.png` ✅ plot 1\n",
                            "- `outputs/task1/viz/task1_daily_pct_change.png` ✅ plot 2\n",
                            "- `outputs/task1/viz/task1_rolling_mean_std.png` ✅ plot 3\n",
                            "- `data/task1/processed/task1_outliers.csv` ✅ Outlier evidence\n",
                            "- `outputs/task1/viz/task1_outliers_returns.png` ✅ Outlier plot\n"
                     ]
              },
              {
                     "cell_type": "code",
                     "execution_count": null,
                     "id": "ddb0592c",
                     "metadata": {},
                     "outputs": [],
                     "source": [
                            "import os\n",
                            "import sys\n",
                            "from pathlib import Path\n",
                            "import importlib\n",
                            "\n",
                            "import pandas as pd\n",
                            "import matplotlib.pyplot as plt\n",
                            "\n",
                            "\n",
                            "def _find_repo_root(start: Path) -> Path:\n",
                            "    \"\"\"Walk upward until we find a folder containing both `src/` and `outputs/` or `data/`.\"\"\"\n",
                            "    start = start.resolve()\n",
                            "    for candidate in [start, *start.parents]:\n",
                            "        if (candidate / \"src\").is_dir() and ((candidate / \"outputs\").exists() or (candidate / \"data\").exists()):\n",
                            "            return candidate\n",
                            "    return start\n",
                            "\n",
                            "\n",
                            "REPO_ROOT = _find_repo_root(Path.cwd())\n",
                            "\n",
                            "# Make imports and relative paths work consistently from notebooks/\n",
                            "if str(REPO_ROOT) not in sys.path:\n",
                            "    sys.path.insert(0, str(REPO_ROOT))\n",
                            "os.chdir(REPO_ROOT)\n",
                            "\n",
                            "from src import config as config\n",
                            "config = importlib.reload(config)\n",
                            "\n",
                            "print(\"Repo root:\", REPO_ROOT)\n",
                            "print(\"Notebook working directory (after chdir):\", os.getcwd())\n",
                            "print(\"config file:\", getattr(config, \"__file__\", None))\n",
                            "print(\"PRICES_PATH:\", config.PRICES_PATH)\n",
                            "print(\"RETURNS_PATH:\", config.RETURNS_PATH)\n",
                            "print(\"TASK1_SCALED_PRICES_PATH:\", getattr(config, \"TASK1_SCALED_PRICES_PATH\", None))\n",
                            "print(\"TASK1_VIZ_DIR:\", getattr(config, \"TASK1_VIZ_DIR\", None))\n"
                     ]
              },
              {
                     "cell_type": "markdown",
                     "id": "da51c9f7",
                     "metadata": {},
                     "source": [
                            "## 1) Load Task 1 datasets (prices + returns)\n",
                            "\n",
                            "These should already exist if you ran your Task 1 pipeline scripts."
                     ]
              },
              {
                     "cell_type": "code",
                     "execution_count": null,
                     "id": "8d93fb27",
                     "metadata": {},
                     "outputs": [],
                     "source": [
                            "prices = pd.read_parquet(config.PRICES_PATH)\n",
                            "returns = pd.read_parquet(config.RETURNS_PATH)\n",
                            "\n",
                            "display(prices.head())\n",
                            "display(returns.head())\n",
                            "print('prices shape:', prices.shape)\n",
                            "print('returns shape:', returns.shape)"
                     ]
              },
              {
                     "cell_type": "code",
                     "execution_count": null,
                     "id": "c82de0ef",
                     "metadata": {},
                     "outputs": [],
                     "source": [
                            "# 1b) Quick EDA summary table (visible evidence)\n",
                            "\n",
                            "ret_col = \"return\" if \"return\" in returns.columns else None\n",
                            "if ret_col:\n",
                            "    summary = (returns.groupby(\"asset\")[ret_col]\n",
                            "               .agg(count=\"count\", mean=\"mean\", std=\"std\", min=\"min\",\n",
                            "                    q01=lambda s: s.quantile(0.01),\n",
                            "                    q99=lambda s: s.quantile(0.99),\n",
                            "                    max=\"max\")\n",
                            "               .reset_index())\n",
                            "    display(summary)"
                     ]
              },
              {
                     "cell_type": "code",
                     "execution_count": null,
                     "id": "6d8f25d0",
                     "metadata": {},
                     "outputs": [],
                     "source": [
                            "# 1c) Outlier detection evidence (required)\n",
                            "\n",
                            "outliers_path = getattr(config, \"TASK1_OUTLIERS_PATH\", None)\n",
                            "if outliers_path is None:\n",
                            "    outliers_path = str(REPO_ROOT / \"data\" / \"task1\" /\n",
                            "                        \"processed\" / \"task1_outliers.csv\")\n",
                            "\n",
                            "if not os.path.exists(outliers_path):\n",
                            "    raise FileNotFoundError(\n",
                            "        f\"Missing outlier evidence file: {outliers_path}. \"\n",
                            "        \"Generate it in Task 1 scripts (recommended) and commit it.\"\n",
                            "    )\n",
                            "\n",
                            "outliers = pd.read_csv(outliers_path)\n",
                            "print(\"Outliers file:\", outliers_path)\n",
                            "print(\"outliers shape:\", outliers.shape)\n",
                            "\n",
                            "# Show counts per asset + sample rows\n",
                            "if \"asset\" in outliers.columns:\n",
                            "    display(outliers[\"asset\"].value_counts().rename(\n",
                            "        \"outlier_count\").to_frame())\n",
                            "\n",
                            "# Show “most extreme” first if you have a score column\n",
                            "score_cols = [c for c in [\"z_score\", \"abs_z\",\n",
                            "                          \"abs_return\", \"return\"] if c in outliers.columns]\n",
                            "if score_cols:\n",
                            "    c = score_cols[0]\n",
                            "    display(outliers.sort_values(c, ascending=False).head(20))\n",
                            "else:\n",
                            "    display(outliers.head(20))\n",
                            "\n",
                            "print(\n",
                            "    \"Method note: Outliers are detected on daily returns using a fixed threshold \"\n",
                            "    \"(e.g., |z-score| > 3 or IQR rule). See scripts/ + README for the exact rule.\"\n",
                            ")"
                     ]
              },
              {
                     "cell_type": "code",
                     "execution_count": null,
                     "id": "3fcb0464",
                     "metadata": {},
                     "outputs": [],
                     "source": [
                            "# 1d) Outlier visualization (required)\n",
                            "\n",
                            "from IPython.display import Image, display\n",
                            "outlier_fig = str(REPO_ROOT / \"outputs\" / \"task1\" /\n",
                            "                  \"viz\" / \"task1_outliers_returns.png\")\n",
                            "os.makedirs(os.path.dirname(outlier_fig), exist_ok=True)\n",
                            "\n",
                            "# Requires columns: date, asset, and return (or pct_change)\n",
                            "returns_ = returns.copy()\n",
                            "returns_[\"date\"] = pd.to_datetime(returns_[\"date\"])\n",
                            "\n",
                            "ret_col = \"return\" if \"return\" in returns_.columns else None\n",
                            "if ret_col is None:\n",
                            "    raise ValueError(\n",
                            "        \"Expected `return` column in returns.parquet for outlier visualization.\")\n",
                            "\n",
                            "# Merge flags (assumes outliers has date/asset)\n",
                            "outliers_ = outliers.copy()\n",
                            "if \"date\" in outliers_.columns:\n",
                            "    outliers_[\"date\"] = pd.to_datetime(outliers_[\"date\"])\n",
                            "\n",
                            "flag_cols = [c for c in outliers_.columns if c.lower(\n",
                            ") in [\"is_outlier\", \"outlier_flag\", \"flag\"]]\n",
                            "flag_col = flag_cols[0] if flag_cols else None\n",
                            "\n",
                            "m = returns_.merge(\n",
                            "    outliers_[[\"date\", \"asset\"] + ([flag_col] if flag_col else [])],\n",
                            "    on=[\"date\", \"asset\"],\n",
                            "    how=\"left\"\n",
                            ")\n",
                            "m[\"is_outlier\"] = m[flag_col].fillna(True) if flag_col else m[[\"date\", \"asset\"]].merge(\n",
                            "    outliers_[[\"date\", \"asset\"]].assign(is_outlier=True),\n",
                            "    on=[\"date\", \"asset\"], how=\"left\"\n",
                            ")[\"is_outlier\"].fillna(False)\n",
                            "\n",
                            "# Plot per asset\n",
                            "assets = sorted(m[\"asset\"].unique())\n",
                            "fig, axes = plt.subplots(len(assets), 1, figsize=(\n",
                            "    12, 3*len(assets)), sharex=True)\n",
                            "if len(assets) == 1:\n",
                            "    axes = [axes]\n",
                            "\n",
                            "for ax, a in zip(axes, assets):\n",
                            "    g = m[m[\"asset\"] == a].sort_values(\"date\")\n",
                            "    ax.plot(g[\"date\"], g[ret_col], linewidth=1.0, label=f\"{a} returns\")\n",
                            "    go = g[g[\"is_outlier\"]]\n",
                            "    ax.scatter(go[\"date\"], go[ret_col], s=25,\n",
                            "               color=\"red\", label=\"outliers\", zorder=3)\n",
                            "    ax.axhline(0, color=\"black\", linewidth=1, alpha=0.6)\n",
                            "    ax.set_title(f\"{a}: returns with outliers highlighted\")\n",
                            "    ax.grid(True, alpha=0.3)\n",
                            "    ax.legend()\n",
                            "\n",
                            "plt.tight_layout()\n",
                            "plt.savefig(outlier_fig, dpi=150)\n",
                            "plt.close(fig)\n",
                            "\n",
                            "print(\"Saved outlier visualization:\", outlier_fig)\n",
                            "\n",
                            "display(Image(filename=outlier_fig))"
                     ]
              },
              {
                     "cell_type": "markdown",
                     "id": "e04ea13e",
                     "metadata": {},
                     "source": [
                            "## 2) Scaling / normalization evidence\n",
                            "\n",
                            "Rubric requirement: demonstrate scaling/normalization. Preferred evidence: a saved scaled dataset.\n",
                            "\n",
                            "This notebook will **auto-generate** a scaled dataset if it does not exist yet (see the next cell), and save it to `config.TASK1_SCALED_PRICES_PATH`.\n"
                     ]
              },
              {
                     "cell_type": "code",
                     "execution_count": null,
                     "id": "0a286d82",
                     "metadata": {},
                     "outputs": [],
                     "source": [
                            "# Scaling/normalization evidence\n",
                            "# If a scaled dataset exists, load it; otherwise generate it from `prices`.\n",
                            "\n",
                            "scaled_path = getattr(config, 'TASK1_SCALED_PRICES_PATH', None)\n",
                            "if scaled_path is None:\n",
                            "    # fallback to the repo convention used by this notebook\n",
                            "    scaled_path = str(REPO_ROOT / 'data' / 'task1' / 'processed' / 'scaled_task1_prices.parquet')\n",
                            "\n",
                            "if os.path.exists(scaled_path):\n",
                            "    scaled_prices = pd.read_parquet(scaled_path)\n",
                            "    print('Loaded scaled dataset:', scaled_path)\n",
                            "else:\n",
                            "    if 'prices' not in globals():\n",
                            "        prices = pd.read_parquet(config.PRICES_PATH)\n",
                            "\n",
                            "    price_col = getattr(config, 'PRICE_COL', 'adj_close')\n",
                            "\n",
                            "    def _minmax(s: pd.Series) -> pd.Series:\n",
                            "        s = s.astype(float)\n",
                            "        denom = (s.max() - s.min())\n",
                            "        if denom == 0:\n",
                            "            return s * 0.0\n",
                            "        return (s - s.min()) / denom\n",
                            "\n",
                            "    scaled_prices = prices.copy()\n",
                            "    scaled_prices[f'{price_col}_scaled'] = scaled_prices.groupby('asset')[price_col].transform(_minmax)\n",
                            "\n",
                            "    os.makedirs(os.path.dirname(scaled_path), exist_ok=True)\n",
                            "    scaled_prices.to_parquet(scaled_path, index=False)\n",
                            "    print('Created scaled dataset:', scaled_path)\n",
                            "\n",
                            "display(scaled_prices.head())\n",
                            "print('scaled_prices shape:', scaled_prices.shape)\n",
                            "print('scaled columns:', list(scaled_prices.columns))\n"
                     ]
              },
              {
                     "cell_type": "markdown",
                     "id": "dc8f782e",
                     "metadata": {},
                     "source": [
                            "## 3) Visualizations (must exist as files)\n",
                            "\n",
                            "This section verifies the three plot files are present.\n",
                            "\n",
                            "If any are missing, rerun:\n",
                            "```bash\n",
                            "python scripts/02_task1_scale_and_viz.py\n",
                            "```"
                     ]
              },
              {
                     "cell_type": "code",
                     "execution_count": null,
                     "id": "c7493fe9",
                     "metadata": {},
                     "outputs": [],
                     "source": [
                            "viz_dir = getattr(config, 'TASK1_VIZ_DIR', 'outputs/task1/viz')\n",
                            "os.makedirs(viz_dir, exist_ok=True)\n",
                            "\n",
                            "expected = [\n",
                            "    os.path.join(viz_dir, 'task1_prices_timeseries.png'),\n",
                            "    os.path.join(viz_dir, 'task1_daily_pct_change.png'),\n",
                            "    os.path.join(viz_dir, 'task1_rolling_mean_std.png'),\n",
                            "]\n",
                            "\n",
                            "missing = [p for p in expected if not os.path.exists(p)]\n",
                            "print('Expected plots:')\n",
                            "for p in expected:\n",
                            "    print(' -', p, 'OK' if os.path.exists(p) else 'MISSING')\n",
                            "\n",
                            "if missing:\n",
                            "    print('\\nSome plots are missing; generating them now...')\n",
                            "\n",
                            "    if 'prices' not in globals():\n",
                            "        prices = pd.read_parquet(config.PRICES_PATH)\n",
                            "    if 'returns' not in globals():\n",
                            "        returns = pd.read_parquet(config.RETURNS_PATH)\n",
                            "\n",
                            "    prices['date'] = pd.to_datetime(prices['date'])\n",
                            "    returns['date'] = pd.to_datetime(returns['date'])\n",
                            "\n",
                            "    price_col = getattr(config, 'PRICE_COL', 'adj_close')\n",
                            "\n",
                            "    # 1) Prices time series\n",
                            "    plt.figure(figsize=(12, 5))\n",
                            "    for asset, g in prices.sort_values('date').groupby('asset'):\n",
                            "        plt.plot(g['date'], g[price_col], label=asset, linewidth=1.5)\n",
                            "    plt.title(f'Prices over time ({price_col})')\n",
                            "    plt.xlabel('Date')\n",
                            "    plt.ylabel(price_col)\n",
                            "    plt.grid(True, alpha=0.3)\n",
                            "    plt.legend()\n",
                            "    plt.tight_layout()\n",
                            "    plt.savefig(expected[0], dpi=150)\n",
                            "    plt.close()\n",
                            "\n",
                            "    # 2) Daily percent change\n",
                            "    tmp = None\n",
                            "    if 'return' in returns.columns:\n",
                            "        tmp = returns.copy()\n",
                            "        tmp['pct_change'] = tmp['return'].astype(float) * 100.0\n",
                            "    else:\n",
                            "        tmp = prices.sort_values('date').copy()\n",
                            "        tmp['pct_change'] = tmp.groupby('asset')[price_col].pct_change() * 100.0\n",
                            "\n",
                            "    plt.figure(figsize=(12, 5))\n",
                            "    for asset, g in tmp.groupby('asset'):\n",
                            "        plt.plot(g['date'], g['pct_change'], label=asset, linewidth=1.0)\n",
                            "    plt.title('Daily % change')\n",
                            "    plt.xlabel('Date')\n",
                            "    plt.ylabel('%')\n",
                            "    plt.grid(True, alpha=0.3)\n",
                            "    plt.legend()\n",
                            "    plt.tight_layout()\n",
                            "    plt.savefig(expected[1], dpi=150)\n",
                            "    plt.close()\n",
                            "\n",
                            "    # 3) Rolling mean/std (20D) of daily % change\n",
                            "    window = 20\n",
                            "    tmp2 = tmp.sort_values('date').copy()\n",
                            "    tmp2['roll_mean'] = tmp2.groupby('asset')['pct_change'].transform(lambda s: s.rolling(window).mean())\n",
                            "    tmp2['roll_std'] = tmp2.groupby('asset')['pct_change'].transform(lambda s: s.rolling(window).std())\n",
                            "\n",
                            "    fig, axes = plt.subplots(2, 1, figsize=(12, 7), sharex=True)\n",
                            "    for asset, g in tmp2.groupby('asset'):\n",
                            "        axes[0].plot(g['date'], g['roll_mean'], label=asset, linewidth=1.2)\n",
                            "        axes[1].plot(g['date'], g['roll_std'], label=asset, linewidth=1.2)\n",
                            "\n",
                            "    axes[0].set_title(f'Rolling mean of daily % change ({window}D)')\n",
                            "    axes[1].set_title(f'Rolling std of daily % change ({window}D)')\n",
                            "    for ax in axes:\n",
                            "        ax.grid(True, alpha=0.3)\n",
                            "        ax.legend()\n",
                            "\n",
                            "    axes[1].set_xlabel('Date')\n",
                            "    fig.tight_layout()\n",
                            "    fig.savefig(expected[2], dpi=150)\n",
                            "    plt.close(fig)\n",
                            "\n",
                            "    missing = [p for p in expected if not os.path.exists(p)]\n",
                            "\n",
                            "if missing:\n",
                            "    raise FileNotFoundError(\n",
                            "        'Missing required Task 1 plot files:\\n' + '\\n'.join(missing)\n",
                            "    )\n",
                            "\n",
                            "print('\\nAll required plot files exist.')\n"
                     ]
              },
              {
                     "cell_type": "markdown",
                     "id": "b8b2a8b5",
                     "metadata": {},
                     "source": [
                            "### Display the saved plot images"
                     ]
              },
              {
                     "cell_type": "code",
                     "execution_count": null,
                     "id": "6a931211",
                     "metadata": {},
                     "outputs": [],
                     "source": [
                            "# 3b) EDA — Visual evidence (required)\n",
                            "\n",
                            "from IPython.display import Image, display\n",
                            "\n",
                            "captions = {\n",
                            "    expected[0]: \"Prices over time: shows overall trend/regime shifts and relative price levels across assets.\",\n",
                            "    expected[1]: \"Daily % change: highlights volatility clustering and extreme move days (spikes).\",\n",
                            "    expected[2]: \"Rolling mean/std: shows time-varying volatility and stability (or instability) of returns.\"\n",
                            "}\n",
                            "\n",
                            "for p in expected:\n",
                            "    display(Image(filename=p))\n",
                            "    print(\"Figure:\", os.path.basename(p))\n",
                            "    print(\"Interpretation:\", captions.get(p, \"\"))\n",
                            "    print(\"-\" * 80)"
                     ]
              },
              {
                     "cell_type": "markdown",
                     "id": "1a3eb5ff",
                     "metadata": {},
                     "source": [
                            "## 4) Stationarity evidence (ADF results)\n",
                            "\n",
                            "ADF outputs should be saved to CSV by your Task 1 scripts."
                     ]
              },
              {
                     "cell_type": "code",
                     "execution_count": null,
                     "id": "401c9117",
                     "metadata": {},
                     "outputs": [],
                     "source": [
                            "adf_path = config.TASK1_ADF_PATH\n",
                            "if not os.path.exists(adf_path):\n",
                            "    raise FileNotFoundError(f'Missing ADF results: {adf_path}. Run Task 1 scripts.')\n",
                            "\n",
                            "adf = pd.read_csv(adf_path)\n",
                            "display(adf)\n",
                            "print('ADF file:', adf_path)"
                     ]
              },
              {
                     "cell_type": "markdown",
                     "id": "f55606f4",
                     "metadata": {},
                     "source": [
                            "## 5) Risk metrics evidence\n",
                            "\n",
                            "Risk metrics (e.g., annualized return/volatility, Sharpe, VaR) should be saved by scripts."
                     ]
              },
              {
                     "cell_type": "code",
                     "execution_count": null,
                     "id": "0e2b58ab",
                     "metadata": {},
                     "outputs": [],
                     "source": [
                            "risk_path = config.TASK1_RISK_PATH\n",
                            "if not os.path.exists(risk_path):\n",
                            "    raise FileNotFoundError(f'Missing risk metrics: {risk_path}. Run Task 1 scripts.')\n",
                            "\n",
                            "risk = pd.read_csv(risk_path)\n",
                            "display(risk)\n",
                            "print('Risk metrics file:', risk_path)"
                     ]
              },
              {
                     "cell_type": "markdown",
                     "metadata": {},
                     "source": [
                            "## 6) Quick interpretation (short)\n",
                            "\n",
                            "- Prices are typically non-stationary; returns/log returns tend to be closer to stationary.\n",
                            "- Scaling provides comparable magnitudes across assets for visualization and certain models.\n",
                            "- Risk metrics summarize reward vs risk and tail behavior for each asset."
                     ]
              }
       ],
       "metadata": {
              "kernelspec": {
                     "display_name": "Menv",
                     "language": "python",
                     "name": "python3"
              },
              "language_info": {
                     "codemirror_mode": {
                            "name": "ipython",
                            "version": 3
                     },
                     "file_extension": ".py",
                     "mimetype": "text/x-python",
                     "name": "python",
                     "nbconvert_exporter": "python",
                     "pygments_lexer": "ipython3",
                     "version": "3.13.9"
              }
       },
       "nbformat": 4,
       "nbformat_minor": 5
}
